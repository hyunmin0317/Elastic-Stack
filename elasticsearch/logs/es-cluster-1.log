[2021-12-27T02:42:38,633][INFO ][o.e.n.Node               ] [node-1] version[7.16.2], pid[10291], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/3.10.0-1160.49.1.el7.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.1/17.0.1+12]
[2021-12-27T02:42:38,643][INFO ][o.e.n.Node               ] [node-1] JVM home [/home/choihm9903/elastic/elasticsearch/jdk], using bundled JDK [true]
[2021-12-27T02:42:38,644][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -Xms512m, -Xmx512m, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp/elasticsearch-4153911833177626276, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -XX:MaxDirectMemorySize=268435456, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/home/choihm9903/elastic/elasticsearch, -Des.path.conf=/home/choihm9903/elastic/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2021-12-27T02:42:42,696][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [aggs-matrix-stats]
[2021-12-27T02:42:42,696][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [analysis-common]
[2021-12-27T02:42:42,697][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [constant-keyword]
[2021-12-27T02:42:42,697][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [frozen-indices]
[2021-12-27T02:42:42,697][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-common]
[2021-12-27T02:42:42,698][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-geoip]
[2021-12-27T02:42:42,698][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-user-agent]
[2021-12-27T02:42:42,698][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [kibana]
[2021-12-27T02:42:42,699][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-expression]
[2021-12-27T02:42:42,699][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-mustache]
[2021-12-27T02:42:42,700][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-painless]
[2021-12-27T02:42:42,701][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [legacy-geo]
[2021-12-27T02:42:42,701][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-extras]
[2021-12-27T02:42:42,701][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-version]
[2021-12-27T02:42:42,702][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [parent-join]
[2021-12-27T02:42:42,702][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [percolator]
[2021-12-27T02:42:42,702][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [rank-eval]
[2021-12-27T02:42:42,702][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [reindex]
[2021-12-27T02:42:42,703][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repositories-metering-api]
[2021-12-27T02:42:42,703][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-encrypted]
[2021-12-27T02:42:42,703][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-url]
[2021-12-27T02:42:42,704][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [runtime-fields-common]
[2021-12-27T02:42:42,704][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [search-business-rules]
[2021-12-27T02:42:42,704][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [searchable-snapshots]
[2021-12-27T02:42:42,705][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [snapshot-repo-test-kit]
[2021-12-27T02:42:42,705][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [spatial]
[2021-12-27T02:42:42,705][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transform]
[2021-12-27T02:42:42,705][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transport-netty4]
[2021-12-27T02:42:42,706][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [unsigned-long]
[2021-12-27T02:42:42,706][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vector-tile]
[2021-12-27T02:42:42,706][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vectors]
[2021-12-27T02:42:42,707][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [wildcard]
[2021-12-27T02:42:42,707][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-aggregate-metric]
[2021-12-27T02:42:42,707][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-analytics]
[2021-12-27T02:42:42,708][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async]
[2021-12-27T02:42:42,708][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async-search]
[2021-12-27T02:42:42,708][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-autoscaling]
[2021-12-27T02:42:42,709][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ccr]
[2021-12-27T02:42:42,709][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-core]
[2021-12-27T02:42:42,709][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-data-streams]
[2021-12-27T02:42:42,710][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-deprecation]
[2021-12-27T02:42:42,710][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-enrich]
[2021-12-27T02:42:42,710][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-eql]
[2021-12-27T02:42:42,710][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-fleet]
[2021-12-27T02:42:42,711][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-graph]
[2021-12-27T02:42:42,711][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-identity-provider]
[2021-12-27T02:42:42,711][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ilm]
[2021-12-27T02:42:42,712][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-logstash]
[2021-12-27T02:42:42,712][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ml]
[2021-12-27T02:42:42,712][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-monitoring]
[2021-12-27T02:42:42,713][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ql]
[2021-12-27T02:42:42,713][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-rollup]
[2021-12-27T02:42:42,713][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-security]
[2021-12-27T02:42:42,714][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-shutdown]
[2021-12-27T02:42:42,714][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-sql]
[2021-12-27T02:42:42,717][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-stack]
[2021-12-27T02:42:42,717][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-text-structure]
[2021-12-27T02:42:42,717][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-voting-only-node]
[2021-12-27T02:42:42,718][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]
[2021-12-27T02:42:42,718][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded
[2021-12-27T02:42:42,771][ERROR][o.e.b.Bootstrap          ] [node-1] Exception
java.lang.IllegalStateException: failed to obtain node locks, tried [[/home/choihm9903/elastic/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?
	at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.node.Node.<init>(Node.java:427) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.node.Node.<init>(Node.java:309) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:234) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:234) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:434) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:166) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:157) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:77) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112) [elasticsearch-cli-7.16.2.jar:7.16.2]
	at org.elasticsearch.cli.Command.main(Command.java:77) [elasticsearch-cli-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:122) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:80) [elasticsearch-7.16.2.jar:7.16.2]
[2021-12-27T02:42:42,779][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [node-1] uncaught exception in thread [main]
org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/home/choihm9903/elastic/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?
	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:170) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:157) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112) ~[elasticsearch-cli-7.16.2.jar:7.16.2]
	at org.elasticsearch.cli.Command.main(Command.java:77) ~[elasticsearch-cli-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:122) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:80) ~[elasticsearch-7.16.2.jar:7.16.2]
Caused by: java.lang.IllegalStateException: failed to obtain node locks, tried [[/home/choihm9903/elastic/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?
	at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.node.Node.<init>(Node.java:427) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.node.Node.<init>(Node.java:309) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:234) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:234) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:434) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:166) ~[elasticsearch-7.16.2.jar:7.16.2]
	... 6 more
[2021-12-27T02:43:05,087][INFO ][o.e.n.Node               ] [node-1] version[7.16.2], pid[10505], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/3.10.0-1160.49.1.el7.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.1/17.0.1+12]
[2021-12-27T02:43:05,098][INFO ][o.e.n.Node               ] [node-1] JVM home [/home/choihm9903/elastic/elasticsearch/jdk], using bundled JDK [true]
[2021-12-27T02:43:05,098][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -Xms512m, -Xmx512m, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp/elasticsearch-14318455482348790885, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -XX:MaxDirectMemorySize=268435456, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/home/choihm9903/elastic/elasticsearch, -Des.path.conf=/home/choihm9903/elastic/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2021-12-27T02:43:09,376][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [aggs-matrix-stats]
[2021-12-27T02:43:09,377][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [analysis-common]
[2021-12-27T02:43:09,378][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [constant-keyword]
[2021-12-27T02:43:09,378][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [frozen-indices]
[2021-12-27T02:43:09,378][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-common]
[2021-12-27T02:43:09,378][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-geoip]
[2021-12-27T02:43:09,379][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-user-agent]
[2021-12-27T02:43:09,379][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [kibana]
[2021-12-27T02:43:09,379][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-expression]
[2021-12-27T02:43:09,380][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-mustache]
[2021-12-27T02:43:09,380][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-painless]
[2021-12-27T02:43:09,380][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [legacy-geo]
[2021-12-27T02:43:09,380][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-extras]
[2021-12-27T02:43:09,381][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-version]
[2021-12-27T02:43:09,381][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [parent-join]
[2021-12-27T02:43:09,381][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [percolator]
[2021-12-27T02:43:09,382][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [rank-eval]
[2021-12-27T02:43:09,382][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [reindex]
[2021-12-27T02:43:09,382][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repositories-metering-api]
[2021-12-27T02:43:09,383][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-encrypted]
[2021-12-27T02:43:09,383][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-url]
[2021-12-27T02:43:09,383][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [runtime-fields-common]
[2021-12-27T02:43:09,383][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [search-business-rules]
[2021-12-27T02:43:09,384][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [searchable-snapshots]
[2021-12-27T02:43:09,384][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [snapshot-repo-test-kit]
[2021-12-27T02:43:09,384][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [spatial]
[2021-12-27T02:43:09,385][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transform]
[2021-12-27T02:43:09,385][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transport-netty4]
[2021-12-27T02:43:09,385][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [unsigned-long]
[2021-12-27T02:43:09,385][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vector-tile]
[2021-12-27T02:43:09,386][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vectors]
[2021-12-27T02:43:09,386][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [wildcard]
[2021-12-27T02:43:09,386][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-aggregate-metric]
[2021-12-27T02:43:09,387][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-analytics]
[2021-12-27T02:43:09,387][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async]
[2021-12-27T02:43:09,388][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async-search]
[2021-12-27T02:43:09,388][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-autoscaling]
[2021-12-27T02:43:09,388][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ccr]
[2021-12-27T02:43:09,388][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-core]
[2021-12-27T02:43:09,389][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-data-streams]
[2021-12-27T02:43:09,389][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-deprecation]
[2021-12-27T02:43:09,390][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-enrich]
[2021-12-27T02:43:09,390][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-eql]
[2021-12-27T02:43:09,390][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-fleet]
[2021-12-27T02:43:09,391][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-graph]
[2021-12-27T02:43:09,391][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-identity-provider]
[2021-12-27T02:43:09,391][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ilm]
[2021-12-27T02:43:09,391][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-logstash]
[2021-12-27T02:43:09,392][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ml]
[2021-12-27T02:43:09,392][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-monitoring]
[2021-12-27T02:43:09,393][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ql]
[2021-12-27T02:43:09,393][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-rollup]
[2021-12-27T02:43:09,393][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-security]
[2021-12-27T02:43:09,393][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-shutdown]
[2021-12-27T02:43:09,394][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-sql]
[2021-12-27T02:43:09,394][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-stack]
[2021-12-27T02:43:09,394][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-text-structure]
[2021-12-27T02:43:09,394][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-voting-only-node]
[2021-12-27T02:43:09,395][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]
[2021-12-27T02:43:09,395][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded
[2021-12-27T02:43:09,462][INFO ][o.e.e.NodeEnvironment    ] [node-1] using [1] data paths, mounts [[/ (/dev/sda2)]], net usable_space [12.4gb], net total_space [19.7gb], types [xfs]
[2021-12-27T02:43:09,462][INFO ][o.e.e.NodeEnvironment    ] [node-1] heap size [512mb], compressed ordinary object pointers [true]
[2021-12-27T02:43:09,491][INFO ][o.e.n.Node               ] [node-1] node name [node-1], node ID [JzlJOkAtR-aGWzNgo4i2dA], cluster name [es-cluster-1], roles [transform, data_frozen, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]
[2021-12-27T02:43:17,081][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/10527] [Main.cc@122] controller (64 bit): Version 7.16.2 (Build 77e5cf03d1077d) Copyright (c) 2021 Elasticsearch BV
[2021-12-27T02:43:18,149][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/home/choihm9903/elastic/elasticsearch/config/roles.yml]
[2021-12-27T02:43:19,149][INFO ][o.e.i.g.ConfigDatabases  ] [node-1] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/home/choihm9903/elastic/elasticsearch/config/ingest-geoip] for changes
[2021-12-27T02:43:19,151][INFO ][o.e.i.g.DatabaseNodeService] [node-1] initialized database registry, using geoip-databases directory [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA]
[2021-12-27T02:43:19,983][INFO ][o.e.t.NettyAllocator     ] [node-1] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=4mb, heap_size=512mb}]
[2021-12-27T02:43:20,113][INFO ][o.e.d.DiscoveryModule    ] [node-1] using discovery type [zen] and seed hosts providers [settings]
[2021-12-27T02:43:20,799][INFO ][o.e.g.DanglingIndicesState] [node-1] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2021-12-27T02:43:21,557][INFO ][o.e.n.Node               ] [node-1] initialized
[2021-12-27T02:43:21,558][INFO ][o.e.n.Node               ] [node-1] starting ...
[2021-12-27T02:43:21,573][INFO ][o.e.x.s.c.f.PersistentCache] [node-1] persistent cache index loaded
[2021-12-27T02:43:21,574][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [node-1] deprecation component started
[2021-12-27T02:43:21,736][INFO ][o.e.t.TransportService   ] [node-1] publish_address {10.178.0.2:9301}, bound_addresses {10.178.0.2:9301}, {[::1]:9301}, {127.0.0.1:9301}
[2021-12-27T02:43:21,925][INFO ][o.e.b.BootstrapChecks    ] [node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks
[2021-12-27T02:43:22,135][INFO ][o.e.c.c.Coordinator      ] [node-1] setting initial configuration to VotingConfiguration{JzlJOkAtR-aGWzNgo4i2dA,{bootstrap-placeholder}-node-2,siXJRVXbSyeYY9nPUgdTwg}
[2021-12-27T02:43:22,553][INFO ][o.e.c.s.ClusterApplierService] [node-1] master node changed {previous [], current [{node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}]}, added {{node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}}, term: 25, version: 294, reason: ApplyCommitRequest{term=25, version=294, sourceNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}
[2021-12-27T02:43:22,628][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-Country.mmdb.tmp.gz]
[2021-12-27T02:43:22,630][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-City.mmdb.tmp.gz]
[2021-12-27T02:43:22,631][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-ASN.mmdb.tmp.gz]
[2021-12-27T02:43:22,655][ERROR][o.e.i.g.DatabaseNodeService] [node-1] failed to download database [GeoLite2-Country.mmdb]
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:927) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:761) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:397) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:485) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:283) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:145) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.OriginSettingClient.doExecute(OriginSettingClient.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:392) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:542) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:367) [ingest-geoip-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T02:43:22,663][ERROR][o.e.i.g.DatabaseNodeService] [node-1] failed to download database [GeoLite2-City.mmdb]
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:927) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:761) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:397) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:485) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:283) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:145) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.OriginSettingClient.doExecute(OriginSettingClient.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:392) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:542) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:367) [ingest-geoip-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T02:43:22,662][ERROR][o.e.i.g.DatabaseNodeService] [node-1] failed to download database [GeoLite2-ASN.mmdb]
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:927) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:761) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:397) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:485) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:283) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:145) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.OriginSettingClient.doExecute(OriginSettingClient.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:392) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:542) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:367) [ingest-geoip-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T02:43:22,933][INFO ][o.e.x.s.a.TokenService   ] [node-1] refresh keys
[2021-12-27T02:43:23,271][INFO ][o.e.x.s.a.TokenService   ] [node-1] refreshed keys
[2021-12-27T02:43:23,311][INFO ][o.e.l.LicenseService     ] [node-1] license [5e2f13df-b2e1-439f-90a0-4a52ff3d3277] mode [basic] - valid
[2021-12-27T02:43:23,313][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [node-1] Active license is now [BASIC]; Security is disabled
[2021-12-27T02:43:23,313][WARN ][o.e.x.s.s.SecurityStatusChangeListener] [node-1] Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.
[2021-12-27T02:43:23,341][INFO ][o.e.h.AbstractHttpServerTransport] [node-1] publish_address {10.178.0.2:9201}, bound_addresses {10.178.0.2:9201}, {[::1]:9201}, {127.0.0.1:9201}
[2021-12-27T02:43:23,342][INFO ][o.e.n.Node               ] [node-1] started
[2021-12-27T02:43:24,072][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-Country.mmdb.tmp.gz]
[2021-12-27T02:43:24,073][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-City.mmdb.tmp.gz]
[2021-12-27T02:43:24,074][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-ASN.mmdb.tmp.gz]
[2021-12-27T02:43:24,636][INFO ][o.e.i.g.DatabaseNodeService] [node-1] successfully reloaded changed geoip database file [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-Country.mmdb]
[2021-12-27T02:43:24,733][INFO ][o.e.i.g.DatabaseNodeService] [node-1] successfully reloaded changed geoip database file [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-ASN.mmdb]
[2021-12-27T02:43:26,293][INFO ][o.e.i.g.DatabaseNodeService] [node-1] successfully reloaded changed geoip database file [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-City.mmdb]
[2021-12-27T02:43:54,058][INFO ][o.e.c.s.ClusterApplierService] [node-1] master node changed {previous [{node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}], current []}, term: 25, version: 300, reason: becoming candidate: joinLeaderInTerm
[2021-12-27T02:43:54,084][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=25, optionalJoin=Optional[Join{term=26, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:43:54,724][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=28, optionalJoin=Optional[Join{term=29, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:43:55,279][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=32, optionalJoin=Optional[Join{term=33, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:43:56,671][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=39, optionalJoin=Optional[Join{term=40, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:43:58,136][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=46, optionalJoin=Optional[Join{term=47, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:43:58,997][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=50, optionalJoin=Optional[Join{term=51, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:01,456][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=57, optionalJoin=Optional[Join{term=58, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:03,888][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=65, optionalJoin=Optional[Join{term=66, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:04,058][WARN ][o.e.c.c.JoinHelper       ] [node-1] last failed join attempt was 169ms ago, failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=65, optionalJoin=Optional[Join{term=66, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:04,062][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 66, last-accepted version 301 in term 25
[2021-12-27T02:44:06,313][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=73, optionalJoin=Optional[Join{term=74, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:07,708][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=77, optionalJoin=Optional[Join{term=78, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:08,682][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=78, optionalJoin=Optional[Join{term=79, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:10,844][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=85, optionalJoin=Optional[Join{term=86, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:11,510][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=88, optionalJoin=Optional[Join{term=89, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:12,921][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=93, optionalJoin=Optional[Join{term=94, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:13,613][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=95, optionalJoin=Optional[Join{term=96, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:14,063][WARN ][o.e.c.c.JoinHelper       ] [node-1] last failed join attempt was 2ms ago, failed to join {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=96, optionalJoin=Optional[Join{term=97, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-3][10.178.0.4:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.cluster.coordination.CoordinationStateRejectedException: incoming last accepted version 301 of join higher than current last accepted version 300 in term 25
	at org.elasticsearch.cluster.coordination.CoordinationState.handleJoin(CoordinationState.java:265) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1244) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T02:44:14,064][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 97, last-accepted version 301 in term 25
[2021-12-27T02:44:16,265][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=100, optionalJoin=Optional[Join{term=101, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:17,602][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=105, optionalJoin=Optional[Join{term=106, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:19,251][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=110, optionalJoin=Optional[Join{term=111, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:22,035][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=117, optionalJoin=Optional[Join{term=118, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:23,515][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=120, optionalJoin=Optional[Join{term=121, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:24,065][WARN ][o.e.c.c.JoinHelper       ] [node-1] last failed join attempt was 550ms ago, failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=120, optionalJoin=Optional[Join{term=121, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:24,070][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 122, last-accepted version 301 in term 25
[2021-12-27T02:44:24,892][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=124, optionalJoin=Optional[Join{term=125, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:25,827][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=126, optionalJoin=Optional[Join{term=127, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:27,423][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=129, optionalJoin=Optional[Join{term=130, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:30,537][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=136, optionalJoin=Optional[Join{term=137, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.RemoteTransportException: [node-2][10.178.0.3:9300][internal:cluster/coordination/join]
Caused by: org.elasticsearch.ElasticsearchException: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:49) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:597) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock held by another program: /home/choihm9903/elastic/es-716/data/nodes/0/_state/write.lock
	at org.apache.lucene.store.NativeFSLockFactory.obtainFSLock(NativeFSLockFactory.java:130) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.FSLockFactory.obtainLock(FSLockFactory.java:41) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.store.BaseDirectory.obtainLock(BaseDirectory.java:45) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:923) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.gateway.PersistedClusterStateService.createIndexWriter(PersistedClusterStateService.java:233) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.PersistedClusterStateService.createWriter(PersistedClusterStateService.java:206) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.getWriterSafe(GatewayMetaState.java:588) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.gateway.GatewayMetaState$LucenePersistedState.setCurrentTerm(GatewayMetaState.java:541) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.CoordinationState.handleStartJoin(CoordinationState.java:199) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.joinLeaderInTerm(Coordinator.java:557) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.ensureTermAtLeast(Coordinator.java:549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.handleJoin(Coordinator.java:1230) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.Optional.ifPresent(Optional.java:178) ~[?:?]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:707) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.done(ListenableActionFuture.java:75) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:131) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.AdapterActionFuture.onResponse(AdapterActionFuture.java:58) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.doHandleResponse(InboundHandler.java:340) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.InboundHandler.lambda$handleResponse$1(InboundHandler.java:328) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]
	at java.lang.Thread.run(Thread.java:833) ~[?:?]
[2021-12-27T02:44:34,071][WARN ][o.e.c.c.JoinHelper       ] [node-1] last failed join attempt was 2.7s ago, failed to join {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=134, optionalJoin=Optional[Join{term=135, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}}]}
org.elasticsearch.transport.RemoteTransportException: [node-1][10.178.0.2:9301][internal:cluster/coordination/join]
Caused by: org.elasticsearch.cluster.coordination.CoordinationStateRejectedException: received a newer join from {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}
	at org.elasticsearch.cluster.coordination.JoinHelper$CandidateJoinAccumulator.handleJoinRequest(JoinHelper.java:560) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.processJoinRequest(Coordinator.java:708) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator.lambda$handleJoinRequest$8(Coordinator.java:594) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$MappedActionListener.onResponse(ActionListener.java:101) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.executeListener(ListenableActionFuture.java:89) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ListenableActionFuture.addListener(ListenableActionFuture.java:54) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:633) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.coordination.Coordinator$1.onResponse(Coordinator.java:630) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$DelegatingActionListener.onResponse(ActionListener.java:186) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1471) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1549) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel$1.run(TransportService.java:1534) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) ~[elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T02:44:34,074][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:44:44,075][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:44:54,077][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:45:04,078][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:45:14,080][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:45:24,081][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:45:34,083][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:45:44,084][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{oCUhG4tST2Gb7tKff6ogwA}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:45:54,086][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{oCUhG4tST2Gb7tKff6ogwA}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:46:04,087][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{oCUhG4tST2Gb7tKff6ogwA}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:46:14,088][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{oCUhG4tST2Gb7tKff6ogwA}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:46:24,090][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{oCUhG4tST2Gb7tKff6ogwA}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:46:32,002][INFO ][o.e.n.Node               ] [node-1] stopping ...
[2021-12-27T02:46:32,013][INFO ][o.e.x.w.WatcherService   ] [node-1] stopping watch service, reason [shutdown initiated]
[2021-12-27T02:46:32,014][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/10527] [Main.cc@174] ML controller exiting
[2021-12-27T02:46:32,016][INFO ][o.e.x.m.p.NativeController] [node-1] Native controller process has stopped - no new native processes can be started
[2021-12-27T02:46:32,014][INFO ][o.e.x.w.WatcherLifeCycleService] [node-1] watcher has stopped and shutdown
[2021-12-27T02:46:32,256][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=135, optionalJoin=Optional[Join{term=136, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-2}{SAdVwrWvSPi8vfEkrusXqg}{b6uY_aTnStioQmMlEc4giw}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}]}
org.elasticsearch.transport.NodeDisconnectedException: [node-2][10.178.0.3:9301][internal:cluster/coordination/join] disconnected
[2021-12-27T02:46:32,289][INFO ][o.e.c.c.JoinHelper       ] [node-1] failed to join {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912} with JoinRequest{sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, minimumTerm=138, optionalJoin=Optional[Join{term=139, lastAcceptedTerm=25, lastAcceptedVersion=301, sourceNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}, targetNode={node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}}]}
org.elasticsearch.transport.SendRequestTransportException: [node-1][10.178.0.2:9301][internal:cluster/coordination/join]
	at org.elasticsearch.transport.TransportService.doStop(TransportService.java:348) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:68) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.node.Node.stop(Node.java:1314) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.node.Node.close(Node.java:1333) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:74) [elasticsearch-core-7.16.2.jar:7.16.2]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:116) [elasticsearch-core-7.16.2.jar:7.16.2]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:66) [elasticsearch-core-7.16.2.jar:7.16.2]
	at org.elasticsearch.bootstrap.Bootstrap$4.run(Bootstrap.java:198) [elasticsearch-7.16.2.jar:7.16.2]
Caused by: org.elasticsearch.node.NodeClosedException: node closed {node-1}{JzlJOkAtR-aGWzNgo4i2dA}{6vOTuR7BT7Gln3XIhF8XGg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}{ml.machine_memory=3970019328, xpack.installed=true, transform.node=true, ml.max_open_jobs=512, ml.max_jvm_size=536870912}
	... 8 more
[2021-12-27T02:46:32,309][INFO ][o.e.n.Node               ] [node-1] stopped
[2021-12-27T02:46:32,309][INFO ][o.e.n.Node               ] [node-1] closing ...
[2021-12-27T02:46:32,323][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [node-1] evicted [0] entries from cache after reloading database [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-Country.mmdb]
[2021-12-27T02:46:32,324][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [node-1] evicted [0] entries from cache after reloading database [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-ASN.mmdb]
[2021-12-27T02:46:32,324][INFO ][o.e.i.g.DatabaseReaderLazyLoader] [node-1] evicted [0] entries from cache after reloading database [/tmp/elasticsearch-14318455482348790885/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA/GeoLite2-City.mmdb]
[2021-12-27T02:46:32,325][INFO ][o.e.n.Node               ] [node-1] closed
[2021-12-27T02:47:05,872][INFO ][o.e.n.Node               ] [node-1] version[7.16.2], pid[10764], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/3.10.0-1160.49.1.el7.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.1/17.0.1+12]
[2021-12-27T02:47:05,878][INFO ][o.e.n.Node               ] [node-1] JVM home [/home/choihm9903/elastic/elasticsearch/jdk], using bundled JDK [true]
[2021-12-27T02:47:05,879][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -Xms512m, -Xmx512m, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp/elasticsearch-2883524453167373494, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -XX:MaxDirectMemorySize=268435456, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/home/choihm9903/elastic/elasticsearch, -Des.path.conf=/home/choihm9903/elastic/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2021-12-27T02:47:09,948][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [aggs-matrix-stats]
[2021-12-27T02:47:09,948][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [analysis-common]
[2021-12-27T02:47:09,949][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [constant-keyword]
[2021-12-27T02:47:09,949][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [frozen-indices]
[2021-12-27T02:47:09,949][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-common]
[2021-12-27T02:47:09,949][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-geoip]
[2021-12-27T02:47:09,950][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-user-agent]
[2021-12-27T02:47:09,950][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [kibana]
[2021-12-27T02:47:09,950][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-expression]
[2021-12-27T02:47:09,951][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-mustache]
[2021-12-27T02:47:09,951][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-painless]
[2021-12-27T02:47:09,951][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [legacy-geo]
[2021-12-27T02:47:09,951][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-extras]
[2021-12-27T02:47:09,952][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-version]
[2021-12-27T02:47:09,952][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [parent-join]
[2021-12-27T02:47:09,952][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [percolator]
[2021-12-27T02:47:09,953][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [rank-eval]
[2021-12-27T02:47:09,953][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [reindex]
[2021-12-27T02:47:09,953][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repositories-metering-api]
[2021-12-27T02:47:09,953][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-encrypted]
[2021-12-27T02:47:09,954][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-url]
[2021-12-27T02:47:09,954][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [runtime-fields-common]
[2021-12-27T02:47:09,954][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [search-business-rules]
[2021-12-27T02:47:09,955][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [searchable-snapshots]
[2021-12-27T02:47:09,955][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [snapshot-repo-test-kit]
[2021-12-27T02:47:09,955][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [spatial]
[2021-12-27T02:47:09,956][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transform]
[2021-12-27T02:47:09,956][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transport-netty4]
[2021-12-27T02:47:09,957][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [unsigned-long]
[2021-12-27T02:47:09,957][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vector-tile]
[2021-12-27T02:47:09,957][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vectors]
[2021-12-27T02:47:09,957][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [wildcard]
[2021-12-27T02:47:09,958][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-aggregate-metric]
[2021-12-27T02:47:09,958][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-analytics]
[2021-12-27T02:47:09,958][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async]
[2021-12-27T02:47:09,959][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async-search]
[2021-12-27T02:47:09,959][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-autoscaling]
[2021-12-27T02:47:09,959][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ccr]
[2021-12-27T02:47:09,960][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-core]
[2021-12-27T02:47:09,960][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-data-streams]
[2021-12-27T02:47:09,960][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-deprecation]
[2021-12-27T02:47:09,960][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-enrich]
[2021-12-27T02:47:09,961][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-eql]
[2021-12-27T02:47:09,961][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-fleet]
[2021-12-27T02:47:09,961][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-graph]
[2021-12-27T02:47:09,962][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-identity-provider]
[2021-12-27T02:47:09,962][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ilm]
[2021-12-27T02:47:09,962][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-logstash]
[2021-12-27T02:47:09,962][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ml]
[2021-12-27T02:47:09,963][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-monitoring]
[2021-12-27T02:47:09,963][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ql]
[2021-12-27T02:47:09,963][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-rollup]
[2021-12-27T02:47:09,964][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-security]
[2021-12-27T02:47:09,964][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-shutdown]
[2021-12-27T02:47:09,964][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-sql]
[2021-12-27T02:47:09,965][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-stack]
[2021-12-27T02:47:09,965][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-text-structure]
[2021-12-27T02:47:09,965][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-voting-only-node]
[2021-12-27T02:47:09,965][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]
[2021-12-27T02:47:09,966][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded
[2021-12-27T02:47:10,014][INFO ][o.e.e.NodeEnvironment    ] [node-1] using [1] data paths, mounts [[/ (/dev/sda2)]], net usable_space [12.3gb], net total_space [19.7gb], types [xfs]
[2021-12-27T02:47:10,015][INFO ][o.e.e.NodeEnvironment    ] [node-1] heap size [512mb], compressed ordinary object pointers [true]
[2021-12-27T02:47:10,110][INFO ][o.e.n.Node               ] [node-1] node name [node-1], node ID [JzlJOkAtR-aGWzNgo4i2dA], cluster name [es-cluster-1], roles [transform, data_frozen, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]
[2021-12-27T02:47:17,618][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/10786] [Main.cc@122] controller (64 bit): Version 7.16.2 (Build 77e5cf03d1077d) Copyright (c) 2021 Elasticsearch BV
[2021-12-27T02:47:18,445][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/home/choihm9903/elastic/elasticsearch/config/roles.yml]
[2021-12-27T02:47:19,473][INFO ][o.e.i.g.ConfigDatabases  ] [node-1] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/home/choihm9903/elastic/elasticsearch/config/ingest-geoip] for changes
[2021-12-27T02:47:19,477][INFO ][o.e.i.g.DatabaseNodeService] [node-1] initialized database registry, using geoip-databases directory [/tmp/elasticsearch-2883524453167373494/geoip-databases/JzlJOkAtR-aGWzNgo4i2dA]
[2021-12-27T02:47:20,415][INFO ][o.e.t.NettyAllocator     ] [node-1] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=4mb, heap_size=512mb}]
[2021-12-27T02:47:20,518][INFO ][o.e.d.DiscoveryModule    ] [node-1] using discovery type [zen] and seed hosts providers [settings]
[2021-12-27T02:47:21,293][INFO ][o.e.g.DanglingIndicesState] [node-1] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2021-12-27T02:47:22,099][INFO ][o.e.n.Node               ] [node-1] initialized
[2021-12-27T02:47:22,100][INFO ][o.e.n.Node               ] [node-1] starting ...
[2021-12-27T02:47:22,115][INFO ][o.e.x.s.c.f.PersistentCache] [node-1] persistent cache index loaded
[2021-12-27T02:47:22,116][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [node-1] deprecation component started
[2021-12-27T02:47:22,266][INFO ][o.e.t.TransportService   ] [node-1] publish_address {10.178.0.2:9301}, bound_addresses {10.178.0.2:9301}, {[::1]:9301}, {127.0.0.1:9301}
[2021-12-27T02:47:23,129][INFO ][o.e.b.BootstrapChecks    ] [node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks
[2021-12-27T02:47:23,173][INFO ][o.e.c.c.Coordinator      ] [node-1] cluster UUID [2AXJqMW2R6GFfRpqcx2quQ]
[2021-12-27T02:47:33,194][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:47:43,197][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:47:53,198][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:47:53,210][WARN ][o.e.n.Node               ] [node-1] timed out while waiting for initial discovery state - timeout: 30s
[2021-12-27T02:47:53,230][INFO ][o.e.h.AbstractHttpServerTransport] [node-1] publish_address {10.178.0.2:9201}, bound_addresses {10.178.0.2:9201}, {[::1]:9201}, {127.0.0.1:9201}
[2021-12-27T02:47:53,231][INFO ][o.e.n.Node               ] [node-1] started
[2021-12-27T02:48:03,200][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:48:13,202][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:48:23,204][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:48:33,205][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:48:43,207][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:48:53,209][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:49:03,211][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:49:13,212][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:49:23,214][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:49:33,215][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:49:43,216][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:49:53,218][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:50:03,219][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:50:13,221][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:50:23,223][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:50:33,224][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:50:43,226][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:50:53,227][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:51:03,228][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:51:13,230][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:51:23,231][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:51:33,233][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:51:43,234][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:51:53,235][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:52:03,237][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:52:13,238][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:52:23,239][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:52:33,241][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:52:43,242][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:52:53,243][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:53:03,245][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:53:13,246][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:53:23,248][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:53:33,249][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:53:43,251][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:53:53,252][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:54:03,254][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:54:13,255][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:54:23,256][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:54:33,257][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:54:43,259][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:54:53,260][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:55:03,261][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:55:13,263][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:55:23,264][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:55:33,265][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:55:43,266][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:55:53,268][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:56:03,269][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:56:13,270][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:56:23,272][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:56:33,273][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:56:43,274][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:56:53,276][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:57:03,277][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:57:13,278][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:57:23,280][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:57:33,281][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:57:43,282][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires at least 2 nodes with ids from [nK7uBO7aQOeCZLSFSe47PA, jQrijfdCSlmBxSd9BPWv9g, siXJRVXbSyeYY9nPUgdTwg], have discovered possible quorum [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{SAdVwrWvSPi8vfEkrusXqg}{Rc7lOxvFQjihIlrbnDpx-Q}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{3SUWC6rXSSWioHbyHwxoNQ}{8_uMeP4QTyi2RwyLtQ0ilw}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JzlJOkAtR-aGWzNgo4i2dA}{LqATcfYxRxa8MZxAPW2qoA}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 139, last-accepted version 301 in term 25
[2021-12-27T02:57:47,689][INFO ][o.e.n.Node               ] [node-1] stopping ...
[2021-12-27T02:57:47,693][INFO ][o.e.x.w.WatcherService   ] [node-1] stopping watch service, reason [shutdown initiated]
[2021-12-27T02:57:47,694][INFO ][o.e.x.w.WatcherLifeCycleService] [node-1] watcher has stopped and shutdown
[2021-12-27T02:57:47,695][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/10786] [Main.cc@174] ML controller exiting
[2021-12-27T02:57:47,696][INFO ][o.e.x.m.p.NativeController] [node-1] Native controller process has stopped - no new native processes can be started
[2021-12-27T02:57:47,727][INFO ][o.e.n.Node               ] [node-1] stopped
[2021-12-27T02:57:47,727][INFO ][o.e.n.Node               ] [node-1] closing ...
[2021-12-27T02:57:47,747][INFO ][o.e.n.Node               ] [node-1] closed
[2021-12-27T03:00:09,761][INFO ][o.e.n.Node               ] [node-1] version[7.16.2], pid[11045], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/3.10.0-1160.49.1.el7.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.1/17.0.1+12]
[2021-12-27T03:00:09,767][INFO ][o.e.n.Node               ] [node-1] JVM home [/home/choihm9903/elastic/elasticsearch/jdk], using bundled JDK [true]
[2021-12-27T03:00:09,768][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -Xms512m, -Xmx512m, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp/elasticsearch-5988047015075701378, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -XX:MaxDirectMemorySize=268435456, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/home/choihm9903/elastic/elasticsearch, -Des.path.conf=/home/choihm9903/elastic/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2021-12-27T03:00:13,849][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [aggs-matrix-stats]
[2021-12-27T03:00:13,850][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [analysis-common]
[2021-12-27T03:00:13,854][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [constant-keyword]
[2021-12-27T03:00:13,854][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [frozen-indices]
[2021-12-27T03:00:13,854][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-common]
[2021-12-27T03:00:13,855][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-geoip]
[2021-12-27T03:00:13,855][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-user-agent]
[2021-12-27T03:00:13,855][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [kibana]
[2021-12-27T03:00:13,856][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-expression]
[2021-12-27T03:00:13,856][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-mustache]
[2021-12-27T03:00:13,856][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-painless]
[2021-12-27T03:00:13,857][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [legacy-geo]
[2021-12-27T03:00:13,857][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-extras]
[2021-12-27T03:00:13,857][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-version]
[2021-12-27T03:00:13,857][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [parent-join]
[2021-12-27T03:00:13,858][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [percolator]
[2021-12-27T03:00:13,858][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [rank-eval]
[2021-12-27T03:00:13,858][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [reindex]
[2021-12-27T03:00:13,859][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repositories-metering-api]
[2021-12-27T03:00:13,859][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-encrypted]
[2021-12-27T03:00:13,859][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-url]
[2021-12-27T03:00:13,859][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [runtime-fields-common]
[2021-12-27T03:00:13,860][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [search-business-rules]
[2021-12-27T03:00:13,860][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [searchable-snapshots]
[2021-12-27T03:00:13,860][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [snapshot-repo-test-kit]
[2021-12-27T03:00:13,860][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [spatial]
[2021-12-27T03:00:13,861][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transform]
[2021-12-27T03:00:13,861][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transport-netty4]
[2021-12-27T03:00:13,861][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [unsigned-long]
[2021-12-27T03:00:13,862][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vector-tile]
[2021-12-27T03:00:13,862][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vectors]
[2021-12-27T03:00:13,862][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [wildcard]
[2021-12-27T03:00:13,862][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-aggregate-metric]
[2021-12-27T03:00:13,863][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-analytics]
[2021-12-27T03:00:13,863][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async]
[2021-12-27T03:00:13,863][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async-search]
[2021-12-27T03:00:13,864][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-autoscaling]
[2021-12-27T03:00:13,864][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ccr]
[2021-12-27T03:00:13,866][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-core]
[2021-12-27T03:00:13,867][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-data-streams]
[2021-12-27T03:00:13,867][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-deprecation]
[2021-12-27T03:00:13,867][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-enrich]
[2021-12-27T03:00:13,867][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-eql]
[2021-12-27T03:00:13,868][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-fleet]
[2021-12-27T03:00:13,868][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-graph]
[2021-12-27T03:00:13,868][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-identity-provider]
[2021-12-27T03:00:13,869][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ilm]
[2021-12-27T03:00:13,869][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-logstash]
[2021-12-27T03:00:13,869][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ml]
[2021-12-27T03:00:13,869][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-monitoring]
[2021-12-27T03:00:13,870][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ql]
[2021-12-27T03:00:13,870][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-rollup]
[2021-12-27T03:00:13,870][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-security]
[2021-12-27T03:00:13,870][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-shutdown]
[2021-12-27T03:00:13,871][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-sql]
[2021-12-27T03:00:13,871][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-stack]
[2021-12-27T03:00:13,871][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-text-structure]
[2021-12-27T03:00:13,871][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-voting-only-node]
[2021-12-27T03:00:13,872][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]
[2021-12-27T03:00:13,872][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded
[2021-12-27T03:00:13,920][INFO ][o.e.e.NodeEnvironment    ] [node-1] using [1] data paths, mounts [[/ (/dev/sda2)]], net usable_space [12.3gb], net total_space [19.7gb], types [xfs]
[2021-12-27T03:00:13,920][INFO ][o.e.e.NodeEnvironment    ] [node-1] heap size [512mb], compressed ordinary object pointers [true]
[2021-12-27T03:00:13,948][INFO ][o.e.n.Node               ] [node-1] node name [node-1], node ID [7gSakvqKTVaAjfVBPMUCJg], cluster name [es-cluster-1], roles [transform, data_frozen, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]
[2021-12-27T03:00:22,448][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/11067] [Main.cc@122] controller (64 bit): Version 7.16.2 (Build 77e5cf03d1077d) Copyright (c) 2021 Elasticsearch BV
[2021-12-27T03:00:23,197][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/home/choihm9903/elastic/elasticsearch/config/roles.yml]
[2021-12-27T03:00:24,210][INFO ][o.e.i.g.ConfigDatabases  ] [node-1] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/home/choihm9903/elastic/elasticsearch/config/ingest-geoip] for changes
[2021-12-27T03:00:24,212][INFO ][o.e.i.g.DatabaseNodeService] [node-1] initialized database registry, using geoip-databases directory [/tmp/elasticsearch-5988047015075701378/geoip-databases/7gSakvqKTVaAjfVBPMUCJg]
[2021-12-27T03:00:25,104][INFO ][o.e.t.NettyAllocator     ] [node-1] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=4mb, heap_size=512mb}]
[2021-12-27T03:00:25,196][INFO ][o.e.d.DiscoveryModule    ] [node-1] using discovery type [zen] and seed hosts providers [settings]
[2021-12-27T03:00:25,971][INFO ][o.e.g.DanglingIndicesState] [node-1] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2021-12-27T03:00:26,800][INFO ][o.e.n.Node               ] [node-1] initialized
[2021-12-27T03:00:26,801][INFO ][o.e.n.Node               ] [node-1] starting ...
[2021-12-27T03:00:26,815][INFO ][o.e.x.s.c.f.PersistentCache] [node-1] persistent cache index loaded
[2021-12-27T03:00:26,816][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [node-1] deprecation component started
[2021-12-27T03:00:26,992][INFO ][o.e.t.TransportService   ] [node-1] publish_address {10.178.0.2:9301}, bound_addresses {10.178.0.2:9301}, {[::1]:9301}, {127.0.0.1:9301}
[2021-12-27T03:00:27,343][INFO ][o.e.b.BootstrapChecks    ] [node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks
[2021-12-27T03:00:27,526][INFO ][o.e.c.c.Coordinator      ] [node-1] setting initial configuration to VotingConfiguration{{bootstrap-placeholder}-node-2,siXJRVXbSyeYY9nPUgdTwg,7gSakvqKTVaAjfVBPMUCJg}
[2021-12-27T03:00:37,380][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:00:47,382][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:00:57,383][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:00:57,390][WARN ][o.e.n.Node               ] [node-1] timed out while waiting for initial discovery state - timeout: 30s
[2021-12-27T03:00:57,405][INFO ][o.e.h.AbstractHttpServerTransport] [node-1] publish_address {10.178.0.2:9201}, bound_addresses {10.178.0.2:9201}, {[::1]:9201}, {127.0.0.1:9201}
[2021-12-27T03:00:57,406][INFO ][o.e.n.Node               ] [node-1] started
[2021-12-27T03:01:07,385][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:01:17,388][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:01:27,389][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:01:37,391][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:01:47,392][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:01:57,394][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:02:07,395][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:02:17,397][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:02:27,398][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:02:37,399][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:02:47,401][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:02:57,402][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:03:07,404][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:03:17,405][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:03:27,407][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:03:37,409][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:03:47,410][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:03:57,412][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:04:07,414][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:04:17,416][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [siXJRVXbSyeYY9nPUgdTwg, 7gSakvqKTVaAjfVBPMUCJg], have discovered possible quorum [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:04:26,826][WARN ][o.e.e.NodeEnvironment    ] [node-1] lock assertion failed
java.nio.file.NoSuchFileException: /home/choihm9903/elastic/elasticsearch/data/nodes/0/node.lock
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) ~[?:?]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106) ~[?:?]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) ~[?:?]
	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) ~[?:?]
	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149) ~[?:?]
	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) ~[?:?]
	at java.nio.file.Files.readAttributes(Files.java:1851) ~[?:?]
	at org.apache.lucene.store.NativeFSLockFactory$NativeFSLock.ensureValid(NativeFSLockFactory.java:189) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.env.NodeEnvironment.assertEnvIsLocked(NodeEnvironment.java:1103) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.env.NodeEnvironment.nodeDataPaths(NodeEnvironment.java:865) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.monitorFSHealth(FsHealthService.java:156) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.run(FsHealthService.java:144) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T03:04:26,836][ERROR][o.e.m.f.FsHealthService  ] [node-1] health check failed
java.lang.IllegalStateException: environment is not locked
	at org.elasticsearch.env.NodeEnvironment.assertEnvIsLocked(NodeEnvironment.java:1106) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.env.NodeEnvironment.nodeDataPaths(NodeEnvironment.java:865) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.monitorFSHealth(FsHealthService.java:156) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.run(FsHealthService.java:144) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: java.nio.file.NoSuchFileException: /home/choihm9903/elastic/elasticsearch/data/nodes/0/node.lock
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) ~[?:?]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106) ~[?:?]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) ~[?:?]
	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) ~[?:?]
	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149) ~[?:?]
	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) ~[?:?]
	at java.nio.file.Files.readAttributes(Files.java:1851) ~[?:?]
	at org.apache.lucene.store.NativeFSLockFactory$NativeFSLock.ensureValid(NativeFSLockFactory.java:189) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.env.NodeEnvironment.assertEnvIsLocked(NodeEnvironment.java:1103) ~[elasticsearch-7.16.2.jar:7.16.2]
	... 9 more
[2021-12-27T03:04:27,417][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:04:37,418][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:04:47,419][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:04:51,583][INFO ][o.e.n.Node               ] [node-1] version[7.16.2], pid[11325], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/3.10.0-1160.49.1.el7.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.1/17.0.1+12]
[2021-12-27T03:04:51,590][INFO ][o.e.n.Node               ] [node-1] JVM home [/home/choihm9903/elastic/elasticsearch/jdk], using bundled JDK [true]
[2021-12-27T03:04:51,590][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -Xms512m, -Xmx512m, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp/elasticsearch-1171829508570096686, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -XX:MaxDirectMemorySize=268435456, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/home/choihm9903/elastic/elasticsearch, -Des.path.conf=/home/choihm9903/elastic/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2021-12-27T03:04:55,795][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [aggs-matrix-stats]
[2021-12-27T03:04:55,796][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [analysis-common]
[2021-12-27T03:04:55,796][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [constant-keyword]
[2021-12-27T03:04:55,796][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [frozen-indices]
[2021-12-27T03:04:55,797][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-common]
[2021-12-27T03:04:55,797][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-geoip]
[2021-12-27T03:04:55,797][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-user-agent]
[2021-12-27T03:04:55,798][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [kibana]
[2021-12-27T03:04:55,798][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-expression]
[2021-12-27T03:04:55,798][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-mustache]
[2021-12-27T03:04:55,799][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-painless]
[2021-12-27T03:04:55,799][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [legacy-geo]
[2021-12-27T03:04:55,801][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-extras]
[2021-12-27T03:04:55,801][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-version]
[2021-12-27T03:04:55,802][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [parent-join]
[2021-12-27T03:04:55,802][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [percolator]
[2021-12-27T03:04:55,802][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [rank-eval]
[2021-12-27T03:04:55,803][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [reindex]
[2021-12-27T03:04:55,803][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repositories-metering-api]
[2021-12-27T03:04:55,803][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-encrypted]
[2021-12-27T03:04:55,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-url]
[2021-12-27T03:04:55,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [runtime-fields-common]
[2021-12-27T03:04:55,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [search-business-rules]
[2021-12-27T03:04:55,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [searchable-snapshots]
[2021-12-27T03:04:55,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [snapshot-repo-test-kit]
[2021-12-27T03:04:55,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [spatial]
[2021-12-27T03:04:55,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transform]
[2021-12-27T03:04:55,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transport-netty4]
[2021-12-27T03:04:55,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [unsigned-long]
[2021-12-27T03:04:55,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vector-tile]
[2021-12-27T03:04:55,807][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vectors]
[2021-12-27T03:04:55,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [wildcard]
[2021-12-27T03:04:55,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-aggregate-metric]
[2021-12-27T03:04:55,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-analytics]
[2021-12-27T03:04:55,810][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async]
[2021-12-27T03:04:55,810][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async-search]
[2021-12-27T03:04:55,810][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-autoscaling]
[2021-12-27T03:04:55,810][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ccr]
[2021-12-27T03:04:55,811][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-core]
[2021-12-27T03:04:55,811][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-data-streams]
[2021-12-27T03:04:55,811][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-deprecation]
[2021-12-27T03:04:55,812][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-enrich]
[2021-12-27T03:04:55,812][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-eql]
[2021-12-27T03:04:55,812][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-fleet]
[2021-12-27T03:04:55,813][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-graph]
[2021-12-27T03:04:55,813][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-identity-provider]
[2021-12-27T03:04:55,813][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ilm]
[2021-12-27T03:04:55,814][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-logstash]
[2021-12-27T03:04:55,814][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ml]
[2021-12-27T03:04:55,814][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-monitoring]
[2021-12-27T03:04:55,814][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ql]
[2021-12-27T03:04:55,815][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-rollup]
[2021-12-27T03:04:55,817][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-security]
[2021-12-27T03:04:55,818][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-shutdown]
[2021-12-27T03:04:55,818][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-sql]
[2021-12-27T03:04:55,818][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-stack]
[2021-12-27T03:04:55,818][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-text-structure]
[2021-12-27T03:04:55,819][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-voting-only-node]
[2021-12-27T03:04:55,819][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]
[2021-12-27T03:04:55,820][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded
[2021-12-27T03:04:55,887][INFO ][o.e.e.NodeEnvironment    ] [node-1] using [1] data paths, mounts [[/ (/dev/sda2)]], net usable_space [12.3gb], net total_space [19.7gb], types [xfs]
[2021-12-27T03:04:55,890][INFO ][o.e.e.NodeEnvironment    ] [node-1] heap size [512mb], compressed ordinary object pointers [true]
[2021-12-27T03:04:55,922][INFO ][o.e.n.Node               ] [node-1] node name [node-1], node ID [JNDJHuQFQI2unmjVxTmVtQ], cluster name [es-cluster-1], roles [transform, data_frozen, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]
[2021-12-27T03:04:57,429][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:03,882][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/11347] [Main.cc@122] controller (64 bit): Version 7.16.2 (Build 77e5cf03d1077d) Copyright (c) 2021 Elasticsearch BV
[2021-12-27T03:05:04,782][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/home/choihm9903/elastic/elasticsearch/config/roles.yml]
[2021-12-27T03:05:05,849][INFO ][o.e.i.g.ConfigDatabases  ] [node-1] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/home/choihm9903/elastic/elasticsearch/config/ingest-geoip] for changes
[2021-12-27T03:05:05,851][INFO ][o.e.i.g.DatabaseNodeService] [node-1] initialized database registry, using geoip-databases directory [/tmp/elasticsearch-1171829508570096686/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ]
[2021-12-27T03:05:06,837][INFO ][o.e.t.NettyAllocator     ] [node-1] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=4mb, heap_size=512mb}]
[2021-12-27T03:05:06,934][INFO ][o.e.d.DiscoveryModule    ] [node-1] using discovery type [zen] and seed hosts providers [settings]
[2021-12-27T03:05:07,430][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:07,658][INFO ][o.e.g.DanglingIndicesState] [node-1] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2021-12-27T03:05:08,378][INFO ][o.e.n.Node               ] [node-1] initialized
[2021-12-27T03:05:08,379][INFO ][o.e.n.Node               ] [node-1] starting ...
[2021-12-27T03:05:08,396][INFO ][o.e.x.s.c.f.PersistentCache] [node-1] persistent cache index loaded
[2021-12-27T03:05:08,397][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [node-1] deprecation component started
[2021-12-27T03:05:08,587][INFO ][o.e.t.TransportService   ] [node-1] publish_address {10.178.0.2:9302}, bound_addresses {10.178.0.2:9302}, {[::1]:9302}, {127.0.0.1:9302}
[2021-12-27T03:05:08,792][INFO ][o.e.b.BootstrapChecks    ] [node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks
[2021-12-27T03:05:08,999][INFO ][o.e.c.c.Coordinator      ] [node-1] setting initial configuration to VotingConfiguration{{bootstrap-placeholder}-node-3,JNDJHuQFQI2unmjVxTmVtQ,jQrijfdCSlmBxSd9BPWv9g}
[2021-12-27T03:05:17,431][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:18,815][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:05:27,432][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:28,818][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:05:37,433][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:38,820][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:05:38,823][WARN ][o.e.n.Node               ] [node-1] timed out while waiting for initial discovery state - timeout: 30s
[2021-12-27T03:05:38,841][INFO ][o.e.h.AbstractHttpServerTransport] [node-1] publish_address {10.178.0.2:9202}, bound_addresses {10.178.0.2:9202}, {[::1]:9202}, {127.0.0.1:9202}
[2021-12-27T03:05:38,842][INFO ][o.e.n.Node               ] [node-1] started
[2021-12-27T03:05:47,434][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:48,822][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:05:57,435][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:05:58,823][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:06:07,436][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:06:08,825][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:06:17,437][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:06:18,827][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:06:26,839][ERROR][o.e.m.f.FsHealthService  ] [node-1] health check failed
org.apache.lucene.store.AlreadyClosedException: Underlying file changed by an external force at 2021-12-27T03:04:55.880704944Z, (lock=NativeFSLock(path=/home/choihm9903/elastic/elasticsearch/data/nodes/0/node.lock,impl=sun.nio.ch.FileLockImpl[0:9223372036854775807 exclusive valid],creationTime=2021-12-27T03:00:13.914056036Z))
	at org.apache.lucene.store.NativeFSLockFactory$NativeFSLock.ensureValid(NativeFSLockFactory.java:191) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.env.NodeEnvironment.assertEnvIsLocked(NodeEnvironment.java:1103) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.env.NodeEnvironment.nodeDataPaths(NodeEnvironment.java:865) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.monitorFSHealth(FsHealthService.java:156) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.run(FsHealthService.java:144) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T03:06:27,438][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:06:28,829][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:06:37,439][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:06:38,831][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:06:47,440][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:06:48,832][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:06:57,441][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:06:58,834][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:07:07,442][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:07:08,835][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:07:17,443][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:07:18,837][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:07:27,444][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:07:28,838][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:07:37,445][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:07:38,840][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:07:47,446][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:07:48,842][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:07:57,447][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:07:58,844][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:08:07,447][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:08:08,845][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:08:17,448][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:08:18,847][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:08:26,841][ERROR][o.e.m.f.FsHealthService  ] [node-1] health check failed
org.apache.lucene.store.AlreadyClosedException: Underlying file changed by an external force at 2021-12-27T03:04:55.880704944Z, (lock=NativeFSLock(path=/home/choihm9903/elastic/elasticsearch/data/nodes/0/node.lock,impl=sun.nio.ch.FileLockImpl[0:9223372036854775807 exclusive valid],creationTime=2021-12-27T03:00:13.914056036Z))
	at org.apache.lucene.store.NativeFSLockFactory$NativeFSLock.ensureValid(NativeFSLockFactory.java:191) ~[lucene-core-8.10.1.jar:8.10.1 2f24e6a49d48a032df1f12e146612f59141727a9 - mayyasharipova - 2021-10-12 15:13:05]
	at org.elasticsearch.env.NodeEnvironment.assertEnvIsLocked(NodeEnvironment.java:1103) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.env.NodeEnvironment.nodeDataPaths(NodeEnvironment.java:865) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.monitorFSHealth(FsHealthService.java:156) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.monitor.fs.FsHealthService$FsHealthMonitor.run(FsHealthService.java:144) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.threadpool.Scheduler$ReschedulingRunnable.doRun(Scheduler.java:214) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:777) [elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T03:08:27,449][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:08:28,848][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:08:37,450][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:08:38,850][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:08:47,451][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:08:48,851][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:08:57,452][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:08:58,853][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:09:07,452][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:09:08,855][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:09:17,453][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:09:18,856][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:09:27,454][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:09:28,858][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:09:37,455][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:09:38,859][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:09:47,456][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:09:48,861][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:09:57,457][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:09:58,862][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:10:07,457][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] this node is unhealthy: health check failed due to broken node lock
[2021-12-27T03:10:08,864][WARN ][o.e.c.c.ClusterFormationFailureHelper] [node-1] master not discovered or elected yet, an election requires 2 nodes with ids [JNDJHuQFQI2unmjVxTmVtQ, jQrijfdCSlmBxSd9BPWv9g], have discovered possible quorum [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}, {node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}, {node-2}{jQrijfdCSlmBxSd9BPWv9g}{m-H5IoeYRA-dFakYrsuLng}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{siXJRVXbSyeYY9nPUgdTwg}{GnjaMOcDR4mOJGy5c_7HPQ}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}, {node-1}{7gSakvqKTVaAjfVBPMUCJg}{CmBTOhNlRD2ayCdDbi6ycg}{10.178.0.2}{10.178.0.2:9301}{cdfhilmrstw}, {node-2}{g8OLLSPsQcuaZkmwlDDtcg}{xhNGGWITThCLmEXwcjQ7Ug}{10.178.0.3}{10.178.0.3:9301}{cdfhilmrstw}, {node-3}{tRejosUbRHGxJgosf6mfpA}{cD81699HSEialFsv_pnz4Q}{10.178.0.4}{10.178.0.4:9301}{cdfhilmrstw}]; discovery will continue using [10.178.0.2:9300, 10.178.0.3:9300, 10.178.0.4:9300] from hosts providers and [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] from last-known cluster state; node term 0, last-accepted version 0 in term 0
[2021-12-27T03:10:16,073][INFO ][o.e.x.m.p.NativeController] [node-1] Native controller process has stopped - no new native processes can be started
[2021-12-27T03:10:16,083][INFO ][o.e.x.m.p.NativeController] [node-1] Native controller process has stopped - no new native processes can be started
[2021-12-27T03:10:16,155][INFO ][o.e.n.Node               ] [node-1] stopping ...
[2021-12-27T03:10:16,162][INFO ][o.e.x.w.WatcherService   ] [node-1] stopping watch service, reason [shutdown initiated]
[2021-12-27T03:10:16,163][INFO ][o.e.x.w.WatcherLifeCycleService] [node-1] watcher has stopped and shutdown
[2021-12-27T03:10:16,168][INFO ][o.e.n.Node               ] [node-1] stopping ...
[2021-12-27T03:10:16,180][INFO ][o.e.x.w.WatcherService   ] [node-1] stopping watch service, reason [shutdown initiated]
[2021-12-27T03:10:16,181][INFO ][o.e.x.w.WatcherLifeCycleService] [node-1] watcher has stopped and shutdown
[2021-12-27T03:10:16,235][INFO ][o.e.t.ClusterConnectionManager] [node-1] transport connection to [{node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}] closed by remote
[2021-12-27T03:10:16,237][INFO ][o.e.t.ClusterConnectionManager] [node-1] transport connection to [{node-1}{nK7uBO7aQOeCZLSFSe47PA}{ipjyFKoyQIODmJHkXPnf5A}{10.178.0.2}{10.178.0.2:9300}{cdfhilmrstw}] closed by remote
[2021-12-27T03:10:16,497][INFO ][o.e.t.ClusterConnectionManager] [node-1] transport connection to [{node-1}{JNDJHuQFQI2unmjVxTmVtQ}{NvWx5G_4Thmg9R0ASaLY4Q}{10.178.0.2}{10.178.0.2:9302}{cdfhilmrstw}] closed by remote
[2021-12-27T03:10:16,528][INFO ][o.e.n.Node               ] [node-1] stopped
[2021-12-27T03:10:16,529][INFO ][o.e.n.Node               ] [node-1] closing ...
[2021-12-27T03:10:16,556][INFO ][o.e.n.Node               ] [node-1] closed
[2021-12-27T03:10:16,665][INFO ][o.e.n.Node               ] [node-1] stopped
[2021-12-27T03:10:16,665][INFO ][o.e.n.Node               ] [node-1] closing ...
[2021-12-27T03:10:16,705][INFO ][o.e.n.Node               ] [node-1] closed
[2021-12-27T03:16:33,290][INFO ][o.e.n.Node               ] [node-1] version[7.16.2], pid[1446], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/3.10.0-1160.49.1.el7.x86_64/amd64], JVM[Eclipse Adoptium/OpenJDK 64-Bit Server VM/17.0.1/17.0.1+12]
[2021-12-27T03:16:33,301][INFO ][o.e.n.Node               ] [node-1] JVM home [/home/choihm9903/elastic/elasticsearch/jdk], using bundled JDK [true]
[2021-12-27T03:16:33,301][INFO ][o.e.n.Node               ] [node-1] JVM arguments [-Xshare:auto, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -XX:+ShowCodeDetailsInExceptionMessages, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=SPI,COMPAT, --add-opens=java.base/java.io=ALL-UNNAMED, -Xms512m, -Xmx512m, -XX:+UseG1GC, -Djava.io.tmpdir=/tmp/elasticsearch-13797255067558043274, -XX:+HeapDumpOnOutOfMemoryError, -XX:+ExitOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -XX:MaxDirectMemorySize=268435456, -XX:G1HeapRegionSize=4m, -XX:InitiatingHeapOccupancyPercent=30, -XX:G1ReservePercent=15, -Des.path.home=/home/choihm9903/elastic/elasticsearch, -Des.path.conf=/home/choihm9903/elastic/elasticsearch/config, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=true]
[2021-12-27T03:16:37,790][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [aggs-matrix-stats]
[2021-12-27T03:16:37,791][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [analysis-common]
[2021-12-27T03:16:37,791][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [constant-keyword]
[2021-12-27T03:16:37,791][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [frozen-indices]
[2021-12-27T03:16:37,792][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-common]
[2021-12-27T03:16:37,792][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-geoip]
[2021-12-27T03:16:37,792][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [ingest-user-agent]
[2021-12-27T03:16:37,792][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [kibana]
[2021-12-27T03:16:37,793][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-expression]
[2021-12-27T03:16:37,793][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-mustache]
[2021-12-27T03:16:37,793][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [lang-painless]
[2021-12-27T03:16:37,795][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [legacy-geo]
[2021-12-27T03:16:37,795][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-extras]
[2021-12-27T03:16:37,795][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [mapper-version]
[2021-12-27T03:16:37,796][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [parent-join]
[2021-12-27T03:16:37,796][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [percolator]
[2021-12-27T03:16:37,796][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [rank-eval]
[2021-12-27T03:16:37,797][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [reindex]
[2021-12-27T03:16:37,797][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repositories-metering-api]
[2021-12-27T03:16:37,797][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-encrypted]
[2021-12-27T03:16:37,798][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [repository-url]
[2021-12-27T03:16:37,798][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [runtime-fields-common]
[2021-12-27T03:16:37,798][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [search-business-rules]
[2021-12-27T03:16:37,799][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [searchable-snapshots]
[2021-12-27T03:16:37,799][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [snapshot-repo-test-kit]
[2021-12-27T03:16:37,799][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [spatial]
[2021-12-27T03:16:37,799][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transform]
[2021-12-27T03:16:37,800][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [transport-netty4]
[2021-12-27T03:16:37,800][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [unsigned-long]
[2021-12-27T03:16:37,800][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vector-tile]
[2021-12-27T03:16:37,801][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [vectors]
[2021-12-27T03:16:37,801][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [wildcard]
[2021-12-27T03:16:37,801][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-aggregate-metric]
[2021-12-27T03:16:37,802][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-analytics]
[2021-12-27T03:16:37,803][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async]
[2021-12-27T03:16:37,803][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-async-search]
[2021-12-27T03:16:37,803][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-autoscaling]
[2021-12-27T03:16:37,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ccr]
[2021-12-27T03:16:37,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-core]
[2021-12-27T03:16:37,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-data-streams]
[2021-12-27T03:16:37,804][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-deprecation]
[2021-12-27T03:16:37,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-enrich]
[2021-12-27T03:16:37,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-eql]
[2021-12-27T03:16:37,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-fleet]
[2021-12-27T03:16:37,805][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-graph]
[2021-12-27T03:16:37,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-identity-provider]
[2021-12-27T03:16:37,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ilm]
[2021-12-27T03:16:37,806][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-logstash]
[2021-12-27T03:16:37,807][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ml]
[2021-12-27T03:16:37,807][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-monitoring]
[2021-12-27T03:16:37,807][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-ql]
[2021-12-27T03:16:37,807][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-rollup]
[2021-12-27T03:16:37,808][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-security]
[2021-12-27T03:16:37,808][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-shutdown]
[2021-12-27T03:16:37,808][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-sql]
[2021-12-27T03:16:37,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-stack]
[2021-12-27T03:16:37,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-text-structure]
[2021-12-27T03:16:37,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-voting-only-node]
[2021-12-27T03:16:37,809][INFO ][o.e.p.PluginsService     ] [node-1] loaded module [x-pack-watcher]
[2021-12-27T03:16:37,810][INFO ][o.e.p.PluginsService     ] [node-1] no plugins loaded
[2021-12-27T03:16:37,862][INFO ][o.e.e.NodeEnvironment    ] [node-1] using [1] data paths, mounts [[/ (/dev/sda2)]], net usable_space [12.3gb], net total_space [19.7gb], types [xfs]
[2021-12-27T03:16:37,877][INFO ][o.e.e.NodeEnvironment    ] [node-1] heap size [512mb], compressed ordinary object pointers [true]
[2021-12-27T03:16:37,953][INFO ][o.e.n.Node               ] [node-1] node name [node-1], node ID [JNDJHuQFQI2unmjVxTmVtQ], cluster name [es-cluster-1], roles [transform, data_frozen, master, remote_cluster_client, data, ml, data_content, data_hot, data_warm, data_cold, ingest]
[2021-12-27T03:16:45,684][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/1468] [Main.cc@122] controller (64 bit): Version 7.16.2 (Build 77e5cf03d1077d) Copyright (c) 2021 Elasticsearch BV
[2021-12-27T03:16:46,329][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/home/choihm9903/elastic/elasticsearch/config/roles.yml]
[2021-12-27T03:16:47,412][INFO ][o.e.i.g.ConfigDatabases  ] [node-1] initialized default databases [[GeoLite2-Country.mmdb, GeoLite2-City.mmdb, GeoLite2-ASN.mmdb]], config databases [[]] and watching [/home/choihm9903/elastic/elasticsearch/config/ingest-geoip] for changes
[2021-12-27T03:16:47,417][INFO ][o.e.i.g.DatabaseNodeService] [node-1] initialized database registry, using geoip-databases directory [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ]
[2021-12-27T03:16:48,405][INFO ][o.e.t.NettyAllocator     ] [node-1] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=4mb, heap_size=512mb}]
[2021-12-27T03:16:48,551][INFO ][o.e.d.DiscoveryModule    ] [node-1] using discovery type [zen] and seed hosts providers [settings]
[2021-12-27T03:16:49,252][INFO ][o.e.g.DanglingIndicesState] [node-1] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
[2021-12-27T03:16:50,016][INFO ][o.e.n.Node               ] [node-1] initialized
[2021-12-27T03:16:50,017][INFO ][o.e.n.Node               ] [node-1] starting ...
[2021-12-27T03:16:50,034][INFO ][o.e.x.s.c.f.PersistentCache] [node-1] persistent cache index loaded
[2021-12-27T03:16:50,035][INFO ][o.e.x.d.l.DeprecationIndexingComponent] [node-1] deprecation component started
[2021-12-27T03:16:50,205][INFO ][o.e.t.TransportService   ] [node-1] publish_address {10.178.0.2:9300}, bound_addresses {10.178.0.2:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2021-12-27T03:16:50,454][INFO ][o.e.b.BootstrapChecks    ] [node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks
[2021-12-27T03:16:50,910][INFO ][o.e.c.s.ClusterApplierService] [node-1] master node changed {previous [], current [{node-2}{H6Q5AlwxRbmLJiyiVjeiKA}{irW4D-orTHSN0vAkkczmzA}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}]}, added {{node-2}{H6Q5AlwxRbmLJiyiVjeiKA}{irW4D-orTHSN0vAkkczmzA}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}, {node-3}{NuSQ1pfPRb6wG_YLhtrPkA}{jofAUVErTee3xggTGZ_MwA}{10.178.0.4}{10.178.0.4:9300}{cdfhilmrstw}}, term: 1, version: 67, reason: ApplyCommitRequest{term=1, version=67, sourceNode={node-2}{H6Q5AlwxRbmLJiyiVjeiKA}{irW4D-orTHSN0vAkkczmzA}{10.178.0.3}{10.178.0.3:9300}{cdfhilmrstw}{ml.machine_memory=3970011136, ml.max_open_jobs=512, xpack.installed=true, ml.max_jvm_size=536870912, transform.node=true}}
[2021-12-27T03:16:50,947][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-Country.mmdb.tmp.gz]
[2021-12-27T03:16:50,948][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-City.mmdb.tmp.gz]
[2021-12-27T03:16:50,949][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-ASN.mmdb.tmp.gz]
[2021-12-27T03:16:50,963][ERROR][o.e.i.g.DatabaseNodeService] [node-1] failed to download database [GeoLite2-Country.mmdb]
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:927) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:761) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:397) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:485) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:283) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:145) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.OriginSettingClient.doExecute(OriginSettingClient.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:392) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:542) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:367) [ingest-geoip-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T03:16:50,968][ERROR][o.e.i.g.DatabaseNodeService] [node-1] failed to download database [GeoLite2-City.mmdb]
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:927) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:761) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:397) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:485) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:283) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:145) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.OriginSettingClient.doExecute(OriginSettingClient.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:392) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:542) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:367) [ingest-geoip-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T03:16:50,967][ERROR][o.e.i.g.DatabaseNodeService] [node-1] failed to download database [GeoLite2-ASN.mmdb]
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:165) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:927) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:761) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$6(TransportSearchAction.java:397) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:112) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:485) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:283) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:145) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:154) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:82) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:95) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:73) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:57) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.OriginSettingClient.doExecute(OriginSettingClient.java:43) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:407) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:392) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:542) ~[elasticsearch-7.16.2.jar:7.16.2]
	at org.elasticsearch.ingest.geoip.DatabaseNodeService.lambda$retrieveDatabase$11(DatabaseNodeService.java:367) [ingest-geoip-7.16.2.jar:7.16.2]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:718) [elasticsearch-7.16.2.jar:7.16.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
[2021-12-27T03:16:51,225][INFO ][o.e.x.s.a.TokenService   ] [node-1] refresh keys
[2021-12-27T03:16:51,505][INFO ][o.e.x.s.a.TokenService   ] [node-1] refreshed keys
[2021-12-27T03:16:51,549][INFO ][o.e.l.LicenseService     ] [node-1] license [45360688-e71a-48cb-a537-6627e1598c93] mode [basic] - valid
[2021-12-27T03:16:51,551][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [node-1] Active license is now [BASIC]; Security is disabled
[2021-12-27T03:16:51,551][WARN ][o.e.x.s.s.SecurityStatusChangeListener] [node-1] Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.16/security-minimal-setup.html to enable security.
[2021-12-27T03:16:51,596][INFO ][o.e.h.AbstractHttpServerTransport] [node-1] publish_address {10.178.0.2:9200}, bound_addresses {10.178.0.2:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2021-12-27T03:16:51,597][INFO ][o.e.n.Node               ] [node-1] started
[2021-12-27T03:16:51,683][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-Country.mmdb] to [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-Country.mmdb.tmp.gz]
[2021-12-27T03:16:51,684][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-City.mmdb] to [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-City.mmdb.tmp.gz]
[2021-12-27T03:16:51,685][INFO ][o.e.i.g.DatabaseNodeService] [node-1] downloading geoip database [GeoLite2-ASN.mmdb] to [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-ASN.mmdb.tmp.gz]
[2021-12-27T03:16:52,016][INFO ][o.e.i.g.DatabaseNodeService] [node-1] successfully reloaded changed geoip database file [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-Country.mmdb]
[2021-12-27T03:16:52,155][INFO ][o.e.i.g.DatabaseNodeService] [node-1] successfully reloaded changed geoip database file [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-ASN.mmdb]
[2021-12-27T03:16:53,907][INFO ][o.e.i.g.DatabaseNodeService] [node-1] successfully reloaded changed geoip database file [/tmp/elasticsearch-13797255067558043274/geoip-databases/JNDJHuQFQI2unmjVxTmVtQ/GeoLite2-City.mmdb]
[2021-12-27T03:31:08,391][INFO ][o.e.t.LoggingTaskListener] [node-1] 6179 finished with response BulkByScrollResponse[took=176.2ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2021-12-27T03:31:08,545][INFO ][o.e.t.LoggingTaskListener] [node-1] 6187 finished with response BulkByScrollResponse[took=717ms,timed_out=false,sliceId=null,updated=247,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2021-12-27T03:34:01,110][INFO ][o.e.t.LoggingTaskListener] [node-1] 6791 finished with response BulkByScrollResponse[took=85.1ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2021-12-27T03:34:01,519][INFO ][o.e.t.LoggingTaskListener] [node-1] 6789 finished with response BulkByScrollResponse[took=514.9ms,timed_out=false,sliceId=null,updated=247,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2021-12-27T03:35:45,193][INFO ][o.e.t.LoggingTaskListener] [node-1] 7327 finished with response BulkByScrollResponse[took=66.5ms,timed_out=false,sliceId=null,updated=17,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
[2021-12-27T03:35:45,561][INFO ][o.e.t.LoggingTaskListener] [node-1] 7329 finished with response BulkByScrollResponse[took=433.5ms,timed_out=false,sliceId=null,updated=247,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
